{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>EBI infrastructure maintains the log files for file downloads for each service running in the EBI infrastructure. The purpose of this workflow is to get the statistics around file downloads to help understand the usage of the files and the projects, and helps to make decisions. </p>"},{"location":"get_started/installation/","title":"Installation","text":""},{"location":"get_started/installation/#a-requirements","title":"A. Requirements","text":"<p>\ud83d\udd39 System Requirements</p> <ol> <li> <p>Operating System:</p> <ul> <li>Linux (preferred)</li> <li>macOS (works with Conda)</li> <li>Windows (with WSL2)</li> </ul> </li> <li> <p>Software Dependencies:</p> <ul> <li>conda (Miniconda or Anaconda)</li> <li>mamba (installed via Conda, if missing)</li> <li>make (for running the Makefile)</li> <li>bash (since Makefile runs Bash scripts)</li> <li>git (recommended for version control)</li> </ul> </li> </ol>"},{"location":"get_started/installation/#b-how-to-install-run-from-local-environment","title":"B. \ud83d\udcd6 How to Install &amp; Run from Local Environment","text":"<p>1\ufe0f\u20e3 Clone the Repository    Clone the repository and go to the location where you installed the pipeline.     <pre><code>git clone git@github.com:PRIDE-Archive/file-download-stat.git\ncd file-download-stat\n</code></pre></p> <p>2\ufe0f\u20e3 Run the Installer    Run the following command to set up the environment and install dependencies:    <pre><code>make setup      # Set up required variables\nmake install    # Run all checks and install dependencies\n</code></pre>    This will:</p> <ul> <li>Set up environment variables</li> <li>Install Conda if missing</li> <li>Install the required Conda environment (file_download_stat)</li> <li>Verify necessary paths</li> </ul> <p>3\ufe0f\u20e3 Copy Log Files    Copy log files from the EBI server to your local machine.     <pre><code> scp -r &lt;username&gt;@&lt;server&gt;:&lt;path_to_log_files&gt; &lt;LOGS_DESTINATION_ROOT&gt;\n</code></pre> 4\ufe0f\u20e3 Run the Pipeline    After the installation is complete, run the pipeline with:    <pre><code>scripts/run_stat_local.sh local \n</code></pre></p>"},{"location":"get_started/installation/#c-how-to-install-run-from-ebi-infrastructure","title":"C. \ud83d\udcd6How to Install &amp; Run from EBI Infrastructure","text":"<p>1\ufe0f\u20e3 Fork the Repository    Fork the file-download-stat repository to your GitHub account.</p> <p>2\ufe0f\u20e3 Set Up EBI GitLab Repo    Set up a repository in GitLab to mirror the repository you forked from GitHub.</p> <p>3\ufe0f\u20e3 Customize <code>params.config</code> File    In the <code>params</code> folder, customize or add your <code>&lt;resource_name&gt;-&lt;environment&gt;-params.config</code> file to configure your pipeline parameters.    For more details, visit parameters.</p> <p>4\ufe0f\u20e3 Set Up CI/CD Pipeline and Variables    Set up the CI/CD pipeline in GitLab. Be sure to configure any necessary environment variables that are required for your pipeline.    For more details, visit GitLab CI/CD variables.</p> <p>5\ufe0f\u20e3 Deploy the Pipeline from GitLab    Deploy the pipeline from GitLab, making sure that the pipeline correctly uses your configuration.</p> <p>6\ufe0f\u20e3 Run <code>make install</code> Command    Go to the location where you installed the pipeline and run the following command to set up the environment and install dependencies:    <pre><code>make setup      # Set up required variables\nmake install    # Run all checks and install dependencies\n</code></pre></p> <p>7\ufe0f\u20e3 Run the Pipeline    After the installation is complete, run the pipeline with:    <pre><code>./run_download_stat.sh\n</code></pre></p> <p>8\ufe0f\u20e3 Optionally, Set Up Seqera Environment    If needed, you can optionally set up the Seqera environment to monitor the running pipelines. Please contact us for more information.</p>"},{"location":"misc/log-files/","title":"Log files","text":""},{"location":"misc/log-files/#log-files","title":"Log Files","text":"<p>Log files are stored in the EBI infrastructure and are in the format of <code>.gz</code> files. The log files are stored in the following format:</p> <p></p> <p>Log files are originally stored in <code>lts</code> and are copied to our <code>nobackup</code> storage for processing.</p>"},{"location":"misc/workflow/","title":"Workflow","text":""},{"location":"misc/workflow/#workflow-architecture","title":"Workflow Architecture","text":""},{"location":"misc/workflow/#1-retrieve-log-files-get_log_files","title":"1. Retrieve Log Files (<code>get_log_files</code>)","text":"<ul> <li>Identifies and compiles a list of relevant log files from the root directory.</li> <li>Output: <code>file_list.txt</code></li> </ul>"},{"location":"misc/workflow/#2-compute-log-file-statistics-run_log_file_stat","title":"2. Compute Log File Statistics (<code>run_log_file_stat</code>)","text":"<ul> <li>Performs statistical analysis on extracted log files.</li> <li>Output: <code>log_file_statistics.html</code></li> </ul>"},{"location":"misc/workflow/#3-process-log-files-process_log_file","title":"3. Process Log Files (<code>process_log_file</code>)","text":"<ul> <li>Extracts and transforms log data into Parquet format for structured analysis.</li> <li>Output: <code>*.parquet</code> files.</li> </ul>"},{"location":"misc/workflow/#4-merge-parquet-datasets-merge_parquet_files","title":"4. Merge Parquet Datasets (<code>merge_parquet_files</code>)","text":"<ul> <li>Aggregates individual Parquet datasets into a singular consolidated dataset.</li> <li>Output: <code>output_parquet</code></li> </ul>"},{"location":"misc/workflow/#5-analyze-merged-dataset-analyze_parquet_files","title":"5. Analyze Merged Dataset (<code>analyze_parquet_files</code>)","text":"<ul> <li>Conducts comprehensive statistical analysis on the merged dataset.</li> <li>Outputs:</li> <li><code>project_level_download_counts.json</code></li> <li><code>file_level_download_counts.json</code></li> <li><code>project_level_yearly_download_counts.json</code></li> <li><code>project_level_top_download_counts.json</code></li> <li><code>all_data.json</code></li> </ul>"},{"location":"misc/workflow/#6-generate-download-statistics-report-run_file_download_stat","title":"6. Generate Download Statistics Report (<code>run_file_download_stat</code>)","text":"<ul> <li>Produces a visual analytical report based on the processed dataset.</li> <li>Output: <code>file_download_stat.html</code></li> </ul>"},{"location":"misc/workflow/#7-update-project-level-download-metrics-update_project_download_counts","title":"7. Update Project-Level Download Metrics (<code>update_project_download_counts</code>)","text":"<ul> <li>Uploads the aggregated project-level download statistics to a designated database.</li> <li>Output: <code>upload_response_file_downloads_per_project.txt</code></li> </ul>"},{"location":"misc/workflow/#8-update-file-level-download-metrics-update_file_level_download_counts","title":"8. Update File-Level Download Metrics (<code>update_file_level_download_counts</code>)","text":"<ul> <li>Segments large JSON datasets into smaller subsets for database ingestion.</li> <li>Output: Server response files confirming successful uploads.</li> </ul>"},{"location":"misc/workflow/#execution-flow","title":"Execution Flow","text":"<ol> <li>Retrieve log files \u2192 <code>file_list.txt</code></li> <li>Analyze log file statistics \u2192 <code>log_file_statistics.html</code></li> <li>Transform log files \u2192 Parquet dataset</li> <li>Merge datasets \u2192 <code>output_parquet</code></li> <li>Analyze aggregated dataset \u2192 JSON statistics reports</li> <li>Generate statistical visualization \u2192 <code>file_download_stat.html</code></li> <li>Update database (if enabled)</li> <li>Project-level download metrics</li> <li>File-level download metrics (batch processing enabled)</li> </ol>"},{"location":"misc/workflow/#parameters","title":"Parameters","text":"Parameter Description <code>params.root_dir</code> The root directory containing log files <code>params.output_file</code> Designated output filename for the Parquet dataset <code>params.log_file</code> Path to the primary log file <code>params.api_endpoint_file_download_per_project</code> API endpoint for project-level file download statistics <code>params.protocols</code> Protocols considered in the processing pipeline <p>Additional parameters relevant for debugging and report generation: - <code>params.resource_identifiers</code> - <code>params.completeness</code> - <code>params.public_private</code> - <code>params.report_template</code> - <code>params.log_file_batch_size</code> - <code>params.resource_base_url</code> - <code>params.report_copy_filepath</code> - <code>params.skipped_years</code> - <code>params.accession_pattern</code> - <code>params.chunk_size</code> - <code>params.disable_db_update</code> - <code>params.api_endpoint_file_downloads_per_file</code></p>"},{"location":"misc/workflow/#debugging-and-error-handling","title":"Debugging and Error Handling","text":"<ul> <li>The workflow captures session metadata and logs critical information at runtime.</li> <li>Intermediate outputs are generated to facilitate validation and troubleshooting.</li> <li>Fault tolerance is enhanced via retry mechanisms using <code>error_retry_max</code> and <code>error_retry_medium</code> labels.</li> </ul>"},{"location":"misc/workflow/#additional-considerations","title":"Additional Considerations","text":"<ul> <li>The workflow is optimized for high-throughput log processing and large-scale statistical analysis.</li> <li>Database updates can be toggled using the <code>params.disable_db_update</code> flag.</li> <li>Input log files may be in compressed (<code>.gz</code>) or uncompressed (<code>.tsv</code>) format.</li> </ul>"},{"location":"report/report-customization/","title":"Customizations","text":""},{"location":"report/report-customization/#overview","title":"Overview","text":"<p>The File Download Statistics report is designed to provide insights into project-level, trends, regional, and user-level statistics. This report is generated based on a predefined HTML template. However, users can customize the report according to their resource requirements by creating their own HTML template.</p>"},{"location":"report/report-customization/#customizing-the-report","title":"Customizing the Report","text":"<p>You can modify the structure, styling, and content of the report by designing a custom HTML template. The placeholders in the template (e.g., <code>{{project_level_content}}</code>, <code>{{trends_content}}</code>, <code>{{maps_content}}</code>, <code>{{user_content}}</code>) will be dynamically populated with relevant data.</p>"},{"location":"report/report-customization/#steps-to-customize","title":"Steps to Customize:","text":"<ol> <li>Create a New HTML Template</li> <li>Copy the existing template.</li> <li>Modify the HTML structure as per your requirements.</li> <li> <p>Ensure the required placeholders are maintained for dynamic content population.</p> </li> <li> <p>Save the Custom Template</p> </li> <li> <p>Name the template according to your preference (e.g., <code>my_resource_report.html</code>).</p> </li> <li> <p>Specify the Template in Configuration</p> </li> <li> <p>In your YAML configuration file, update the <code>report_template</code> parameter:</p> <p><pre><code>report_template: \"&lt;resource_name&gt;_report.html\"\n</code></pre>    - Replace <code>&lt;resource_name&gt;</code> with the actual name of your custom report template.</p> </li> </ol>"},{"location":"report/report-customization/#example-html-template","title":"Example HTML Template","text":"<p>Below is the default HTML template that can be used as a reference:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\"&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.min.css\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"container\"&gt;\n        &lt;div class=\"center\"&gt;\n            &lt;h1 class=\"black center\" style=\"color: white; padding-top: 15px; padding-bottom: 15px; border-bottom: 4px solid green\"&gt;\n                File Download Statistics\n            &lt;/h1&gt;\n        &lt;/div&gt;\n        &lt;div class=\"content\"&gt;\n            &lt;h4 style=\"border-bottom: 1px solid black; border-top: 1px solid black;\"&gt; 1. Project Level Statistics &lt;/h4&gt;\n            {{project_level_content}}\n            &lt;h4 style=\"border-bottom: 1px solid black; border-top: 1px solid black;\"&gt; 2. Trends Statistics &lt;/h4&gt;\n            {{trends_content}}\n            &lt;h4 style=\"border-bottom: 1px solid black; border-top: 1px solid black;\"&gt; 3. Regional Level Statistics &lt;/h4&gt;\n            {{maps_content}}\n            &lt;h4 style=\"border-bottom: 1px solid black; border-top: 1px solid black;\"&gt; 4. User Level Statistics &lt;/h4&gt;\n            {{user_content}}\n        &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script type=\"text/javascript\" src=\"js/materialize.min.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"report/report-customization/#notes","title":"Notes","text":"<ul> <li>Ensure the HTML template is properly formatted and includes all necessary placeholders.</li> <li>The YAML configuration must point to the correct file name of your custom template.</li> <li>If additional statistics sections are required, update both the template and data sources accordingly.</li> </ul> <p>By following these steps, you can customize the  File Download Statistics report to fit your specific needs and resource requirements.</p>"},{"location":"report/report-interpretation/","title":"Interpretation","text":""},{"location":"report/report-interpretation/#1-project-level-statistics","title":"1. Project Level Statistics","text":""},{"location":"report/report-interpretation/#yearly-total-downloads-separated-by-method-including-totals","title":"Yearly Total Downloads Separated by Method (Including Totals)","text":"<ul> <li>This bar chart displays yearly total downloads, separated by download methods: FTP, HTTP, and GridFTP-Globus.</li> <li>The highest number of downloads occurred in 2022, followed by 2023 and 2024.</li> <li>The GridFTP-Globus method was the most utilized, significantly contributing to the total downloads.</li> </ul>"},{"location":"report/report-interpretation/#monthly-downloads-total-and-by-method","title":"Monthly Downloads: Total and by Method","text":"<ul> <li>This line chart shows monthly download trends from 2021 onward.</li> <li>Peak download activity occurred around late 2022 and early 2023, with noticeable fluctuations across different months.</li> <li>All methods followed similar trends, with GridFTP-Globus consistently being the preferred choice.</li> </ul>"},{"location":"report/report-interpretation/#cumulative-downloads-over-time-month-year","title":"Cumulative Downloads Over Time (Month-Year)","text":"<ul> <li>This plot tracks cumulative downloads over time, showing a steady increase in file retrievals.</li> <li>Significant jumps in cumulative downloads are observed around mid-2022 and early 2023, reflecting major surges in download activity.</li> </ul>"},{"location":"report/report-interpretation/#distribution-of-projects-by-download-count","title":"Distribution of Projects by Download Count","text":"<ul> <li>This scatter plot illustrates the number of projects versus their download count.</li> <li>The majority of projects have fewer than 2,000 downloads, with only a few exceeding 10,000.</li> <li>This suggests that while a few projects are highly popular, most have moderate to low download counts.</li> </ul>"},{"location":"report/report-interpretation/#2-trends-statistics","title":"2. Trends Statistics","text":""},{"location":"report/report-interpretation/#file-downloads-over-time","title":"File Downloads Over Time","text":"<ul> <li>This time-series graph shows daily downloads over the years.</li> <li>A significant spike in downloads is visible around mid-2022, followed by occasional surges in 2023 and 2024.</li> <li>HTTP downloads (red) show the highest fluctuations, while FTP and GridFTP-Globus exhibit steadier trends.</li> </ul>"},{"location":"report/report-interpretation/#3-regional-level-statistics","title":"3. Regional Level Statistics","text":""},{"location":"report/report-interpretation/#downloads-by-country-over-time","title":"Downloads by Country Over Time","text":"<ul> <li>A world map visualizes download distributions across different countries.</li> <li>Larger circles indicate higher download volumes, with notable activity in regions such as North America, Europe, and Asia.</li> <li>The color gradient represents download intensity, with warmer colors indicating higher downloads.</li> </ul>"},{"location":"report/report-interpretation/#4-user-level-statistics","title":"4. User Level Statistics","text":""},{"location":"report/report-interpretation/#unique-users-over-time","title":"Unique Users Over Time","text":"<ul> <li>This time-series graph shows the number of unique users downloading files over time.</li> <li>Similar to file downloads, user activity peaked in mid-2022 and late 2024.</li> <li>The graph indicates an increase in user engagement, especially during specific periods.</li> </ul>"},{"location":"report/report-interpretation/#key-insights","title":"Key Insights","text":"<ul> <li>2022 saw the highest number of downloads, with a sharp increase in activity.</li> <li>GridFTP-Globus remains the most preferred download method.</li> <li>The majority of projects have relatively low download counts, with a few exceptions driving the highest traffic.</li> <li>Download activity is concentrated in North America, Europe, and Asia.</li> <li>User engagement follows similar trends to total downloads, with peaks in 2022 and 2024.</li> </ul>"},{"location":"workflow/gitlab-config/","title":"GitLab CI/CD","text":"<p>This document outlines the environment variables that must be provided in GitLab CI/CD for the pipeline to function correctly. These variables should be configured in the GitLab repository settings under Settings &gt; CI/CD &gt; Variables.</p>"},{"location":"workflow/gitlab-config/#required-environment-variables","title":"Required Environment Variables","text":"Variable Name Description <code>API_ENDPOINT_FILE_DOWNLOADS_PER_FILE</code> API endpoint for updating file-level download statistics. <code>API_ENDPOINT_FILE_DOWNLOADS_PER_PROJECT</code> API endpoint for updating project-level download statistics. <code>API_ENDPOINT_HEADER</code> Authorization header for API requests. <code>CONDA_INIT</code> Path to the Conda initialization script. <code>DATA_ROOT_DIR</code> Root directory where data is stored. <code>DEPLOY_PATH</code> Path where the pipeline artifacts or results will be deployed. <code>DEPLOY_SERVER</code> The server address where deployment occurs. <code>LOG_FOLDER</code> Directory where logs should be stored. <code>NEXTFLOW</code> Path to the Nextflow executable. <code>PIPELINE_BASE_DIR</code> Base directory of the pipeline execution. <code>SERVER_USER</code> Username for the deployment server. <code>SSH_KEY</code> SSH private key for accessing the deployment server. <code>SYMLINK</code> Path to create symbolic links for processed data. <code>WORK_PATH</code> Working directory for pipeline execution."},{"location":"workflow/gitlab-config/#setting-up-variables-in-gitlab-cicd","title":"Setting Up Variables in GitLab CI/CD","text":"<p>To configure these variables: 1. Navigate to your GitLab project. 2. Go to Settings &gt; CI/CD. 3. Expand the Variables section. 4. Click Add Variable and enter the name, value, and choose the appropriate scope and protection. 5. Click Save variables.</p> <p>.gitlab-ci.yml</p> <p>Ensure that your <code>.gitlab-ci.yml</code> file references these variables correctly</p>"},{"location":"workflow/gitlab-config/#security-recommendations","title":"Security Recommendations","text":"<ul> <li>Protect sensitive variables (SSH_KEY, API_ENDPOINT_HEADER) by marking them as protected and masked.</li> <li>Avoid exposing secrets in logs by setting Mask Variable when adding them to GitLab CI/CD.</li> <li>Use SSH keys securely by ensuring SSH_KEY is configured correctly for authentication.</li> </ul>"},{"location":"workflow/gitlab-config/#troubleshooting","title":"Troubleshooting","text":"<p>If any variable is missing or incorrectly set, the pipeline may fail. Check the following:</p> <ul> <li>Ensure all required variables are defined in Settings &gt; CI/CD &gt; Variables.</li> <li>Use echo $VARIABLE_NAME in the pipeline script to debug missing variables.</li> <li>Check GitLab CI/CD logs for any environment variable errors.</li> </ul>"},{"location":"workflow/nextflow-slurm-config/","title":"Nextflow SLURM","text":"<p>This document outlines the SLURM resource configurations and process-specific settings used in Nextflow. It explains CPU, memory, time allocations, and error handling strategies for different process types.</p>"},{"location":"workflow/nextflow-slurm-config/#global-process-configuration","title":"Global Process Configuration","text":"<p>The following global resource allocation settings are applied to all Nextflow processes by default:</p> Parameter Description Computation <code>cpus</code> Number of CPU cores assigned to a task <code>2 * task.attempt</code> <code>memory</code> Amount of memory allocated <code>8 GB * task.attempt</code> <code>time</code> Maximum execution time per task <code>4 h * task.attempt</code> <code>errorStrategy</code> Defines how the workflow handles errors Retries on exit status <code>130-145</code> and <code>104</code>, otherwise finishes <code>maxRetries</code> Maximum number of retry attempts for a task <code>3</code> <code>maxErrors</code> Maximum number of errors allowed before failing <code>-1</code> (Unlimited errors)"},{"location":"workflow/nextflow-slurm-config/#process-specific-resource-allocations","title":"Process-Specific Resource Allocations","text":"<p>Each process label defines custom resource requirements based on workload demands. These labels are commonly used in nf-core DSL2 modules and can be reused in local modules.</p>"},{"location":"workflow/nextflow-slurm-config/#1-single-core-processes-process_single","title":"1. Single-Core Processes (<code>process_single</code>)","text":"Resource Allocation <code>cpus</code> <code>1</code> <code>memory</code> <code>6 GB * task.attempt</code> <code>time</code> <code>4 h * task.attempt</code>"},{"location":"workflow/nextflow-slurm-config/#2-very-low-resource-processes-process_very_low","title":"2. Very Low Resource Processes (<code>process_very_low</code>)","text":"Resource Allocation <code>cpus</code> <code>1 * task.attempt</code> <code>memory</code> <code>1 GB * task.attempt</code> <code>time</code> <code>1 h * task.attempt</code>"},{"location":"workflow/nextflow-slurm-config/#3-low-resource-processes-process_low","title":"3. Low Resource Processes (<code>process_low</code>)","text":"Resource Allocation <code>cpus</code> <code>4 * task.attempt</code> <code>memory</code> <code>12 GB * task.attempt</code> <code>time</code> <code>6 h * task.attempt</code>"},{"location":"workflow/nextflow-slurm-config/#4-medium-resource-processes-process_medium","title":"4. Medium Resource Processes (<code>process_medium</code>)","text":"Resource Allocation <code>cpus</code> <code>8 * task.attempt</code> <code>memory</code> <code>36 GB * task.attempt</code> <code>time</code> <code>8 h * task.attempt</code>"},{"location":"workflow/nextflow-slurm-config/#5-high-resource-processes-process_high","title":"5. High Resource Processes (<code>process_high</code>)","text":"Resource Allocation <code>cpus</code> <code>12 * task.attempt</code> <code>memory</code> <code>72 GB * task.attempt</code> <code>time</code> <code>16 h * task.attempt</code>"},{"location":"workflow/nextflow-slurm-config/#6-long-running-processes-process_long","title":"6. Long-Running Processes (<code>process_long</code>)","text":"Resource Allocation <code>time</code> <code>20 h * task.attempt</code>"},{"location":"workflow/nextflow-slurm-config/#7-high-memory-processes-process_high_memory","title":"7. High-Memory Processes (<code>process_high_memory</code>)","text":"Resource Allocation <code>memory</code> <code>200 GB * task.attempt</code>"},{"location":"workflow/nextflow-slurm-config/#error-handling-strategies","title":"Error Handling Strategies","text":"<p>Some processes have customized error handling strategies:</p>"},{"location":"workflow/nextflow-slurm-config/#ignore-errors-error_ignore","title":"Ignore Errors (<code>error_ignore</code>)","text":"Strategy Action <code>errorStrategy</code> <code>ignore</code>"},{"location":"workflow/nextflow-slurm-config/#retry-errors-error_retry","title":"Retry Errors (<code>error_retry</code>)","text":"Strategy Action <code>errorStrategy</code> <code>retry</code> <code>maxRetries</code> <code>3</code>"},{"location":"workflow/nextflow-slurm-config/#global-parameter-defaults","title":"Global Parameter Defaults","text":"<p>The following default limits are defined for resource usage but can be overwritten when running the workflow.</p> Parameter Default Value <code>max_memory</code> <code>128 GB</code> <code>max_cpus</code> <code>16</code> <code>max_time</code> <code>240 h</code>"},{"location":"workflow/nextflow-slurm-config/#example-configuration-file","title":"Example Configuration File","text":"<pre><code>params {\n    max_memory = 128.GB\n    max_cpus = 16\n    max_time = 240.h\n}\n</code></pre>"},{"location":"workflow/overview/","title":"Overview","text":""},{"location":"workflow/overview/#workflow","title":"Workflow","text":""},{"location":"workflow/overview/#1-copy-data","title":"1. Copy Data","text":"<p>If you are running the pipeline in EBI infrastructure, the pipeline will copy data from the original log file location to your path    Currently, original log files are stored in a place where only <code>datamover</code> can be read. So, as the first step, our pipeline will copy(<code>rsync</code>) the log files to the location you specified which can be accessed by the <code>standard</code> queue.     Once this job is completed, it will automatically launched the next dependant job to process the log files and do the statistical analysis.</p> <p>Running first time</p> <p>It could take 2-3 hours to copy the log files for the first time, then it is will be few minutes for the subsequent runs.</p>"},{"location":"workflow/overview/#2-process-log-files","title":"2. Process Log Files","text":"<p>This step will collect the names of log files, process the log files parallel and apply many filters excluding the unwanted data.     The processed log files will be stored in the Parquet format which is a columnar storage format that is optimized for reading and writing large datasets.</p> <p></p>"},{"location":"workflow/overview/#3-produce-statistics-report","title":"3. Produce Statistics Report","text":"<p>Using dask framework, parquet will be queried and the statistics will be generated.    This step will generate the statistics report in the HTML format and will be stored in the location you specified.</p> <p></p> <p>Detailed workflow steps can be found in the workflow documentation.</p>"},{"location":"workflow/param-config/","title":"Parameters","text":""},{"location":"workflow/param-config/#log-file-parsing","title":"Log File Parsing","text":"<ul> <li><code>protocols</code>   A list of protocols supported for data transfer.</li> <li> <p>Values:</p> <ul> <li><code>fasp-aspera</code>: Aspera FASP protocol.</li> <li><code>gridftp-globus</code>: GridFTP with Globus.</li> <li><code>http</code>: HTTP protocol.</li> <li><code>ftp</code>: FTP protocol.</li> </ul> </li> <li> <p><code>public_private</code>   A list of access types for the dataset.</p> </li> <li> <p>Values:</p> <ul> <li><code>public</code>: Data that is publicly available.</li> <li><code>private</code>: Data that is privately available.</li> </ul> </li> <li> <p><code>resource_identifiers</code>   A list of identifiers that define valid resource locations.</p> </li> <li> <p>Values:</p> <ul> <li><code>/pride/data/archive</code>: Path to PRIDE archive data.</li> <li><code>/pride-archive</code>: Path to an alternate PRIDE archive.</li> </ul> </li> <li> <p><code>accession_pattern</code>   A regular expression pattern to validate the accession IDs of the dataset.</p> </li> <li>Default: <code>'^PXD\\\\d{6}$'</code></li> <li> <p>Explanation: This pattern is designed to match PRIDE accession IDs (e.g., <code>PXD000123</code>).</p> </li> <li> <p><code>completeness</code>   A list of possible data completion statuses.</p> </li> <li> <p>Values:</p> <ul> <li><code>complete</code>: Indicates that the file is complete.</li> <li><code>incomplete</code>: Indicates that the file has not downloaded successfully.</li> </ul> </li> <li> <p><code>log_file_batch_size</code>   The number of rows to process in each batch when reading log files.</p> </li> <li>Default: <code>1000</code></li> <li>Explanation: Controls how many lines are processed in each batch when parsing large log files.</li> </ul>"},{"location":"workflow/param-config/#statistics-reports","title":"Statistics Reports","text":"<ul> <li><code>report_template</code>   The filename of the HTML template to be used for generating reports.</li> <li>Default: <code>\"pride_report.html\"</code></li> <li> <p>Explanation: This template is used for generating a formatted report of the PRIDE data.  However, create your own template for your resource by customizing this template and specify it in the configuration file.</p> </li> <li> <p><code>skipped_years</code>   A list of years for which reports will be skipped.</p> </li> <li> <p>Values: </p> <ul> <li>Example: <code>[2020, 2025]</code> </li> <li>These years will be excluded from the report generation.</li> </ul> </li> <li> <p><code>resource_base_url</code>   The base URL for accessing the resources (e.g., project details).</p> </li> <li>Default: <code>'https://www.ebi.ac.uk/pride/archive/projects/'</code></li> <li> <p>Explanation: This URL is used to link to the resource projects in the PRIDE archive. This is used to link back accessions mentioned in the report to your original dataset in your resource.</p> </li> <li> <p><code>report_copy_filepath</code>   The file path where the report copy will be saved after processing.</p> </li> <li>Default: <code>/your/path/Desktop</code></li> <li>Explanation: The generated report will be saved to this location on the local system.</li> </ul>"},{"location":"workflow/param-config/#push-to-a-database","title":"Push to a Database","text":"<ul> <li><code>disable_db_update</code>   A flag to enable or disable updates to the database(e.g., MongoDB).</li> <li> <p>Values:</p> <ul> <li><code>true</code>: Disable database updates.</li> <li><code>false</code>: Enable database updates.</li> </ul> </li> <li> <p><code>chunk_size</code>   The size of data chunks to be handled during processing.</p> </li> <li>Default: <code>100000</code></li> <li>Explanation: Determines how much data to process at a time, based on this information, file level statistics will be saved into several JSON files to reduce the payload size when you upload them into your database(Ex: MongoDB)</li> </ul>"},{"location":"workflow/param-config/#example-configuration-file","title":"Example Configuration File","text":"<pre><code>############# Log File Parsing #############\n\nprotocols:\n  - fasp-aspera\n  - gridftp-globus\n  - http\n  - ftp\npublic_private:\n  - public\nresource_identifiers:\n  - /pride/data/archive\n  - /pride-archive\naccession_pattern: '^PXD\\\\d{6}$'\ncompleteness:\n  - complete\nlog_file_batch_size: 1000\n\n############# Statistics Reports #############\n\nreport_template: \"pride_report.html\"\nskipped_years:\n  - 2020\n  - 2025\nresource_base_url: 'https://www.ebi.ac.uk/pride/archive/projects/'\nreport_copy_filepath: /your/machine/Desktop\n\n############# Push to a Database(e.g MongoDB) #############\n\ndisable_db_update: true\nchunk_size: 100000\n</code></pre>"}]}